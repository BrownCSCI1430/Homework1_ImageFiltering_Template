%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CSCI 1430 Written Question Template
%
% This is a LaTeX document. LaTeX is a markup language for producing documents. 
% You will fill out this document, compile it into a PDF document, then upload the PDF to Gradescope. 
%
% To compile it into a PDF you can:
% - Use vscode! Install a LaTeX distribution (all common OS): http://www.latex-project.org/get/ 
% + VSCode extension: https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop
% - Use an online tool: https://www.overleaf.com/ - most LaTeX packages are pre-installed here (e.g., \usepackage{}).
%
% If you need help with LaTeX, please come to office hours.
% Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% The CSCI1430 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{csci1430}

\begin{document}
\title{Homework 1 Written Questions}
\HomeworkShortName{HW1}
\maketitle

\writeinstructions

% Please leave the pagebreak
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=12,drawbox=false]
We have been given special permission to use the telescope on the roof of Barus and Holley. Unfortunately, our fantastic image of the Orion nebula has noise: (\href{run:images/orion-noise.png}{orion-noise.png})

\includegraphics[width=0.5\textwidth,height=6cm,keepaspectratio]
{images/orion-noise.png}

One way to deal with this noise is with image convolution.
Convolution is a type of image filtering that is a fundamental image processing tool.
\end{question}

\begin{orangebox}
\emph{Explicitly describe} the input, transformation, and output components of 2D discrete convolution. Please be precise; define variables as need.
\end{orangebox}

\begin{subsubquestion}[points=2]
Input \textbf{[2--4 sentences]}
\end{subsubquestion}

\begin{answer}[height=7]

\end{answer}

\begin{subsubquestion}[points=2]
    Transformation (how is the image transformed?) \textbf{[2--4 sentences]}
\end{subsubquestion}

\begin{answer}[height=9]

\end{answer}

\pagebreak    
\begin{subsubquestion}[points=2]
    Output \textbf{[2--4 sentences]}
\end{subsubquestion}

\begin{answer}[height=7]

\end{answer}


\begin{subquestion}[points=4]
Describe two filter kernels that we may use with convolution, and give an example computer vision application that each enables. \textbf{[4--8 sentences]}
\end{subquestion}

\begin{answer}[height=14]

\end{answer}

\begin{subquestion}[points=2]
What kind of filter might we use to de-noise our image of the Orion nebula, and why? \textbf{[2--3 sentences]}
\end{subquestion}

\begin{answer}[height=7]

\end{answer}

\pagebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=8,drawbox=false]
Now that we've de-noised our image of the Orion nebula, let's explore filtering techniques more closely. Two kinds of linear filtering are correlation and convolution.
\end{question}

\begin{subquestion}[points=3]
    What is different between convolution and correlation? Include differences in their algebraic properties. When might we use each? \textbf{[5--6 sentences]}
\end{subquestion}    
    
\begin{answer}[height=14]
    
\end{answer}
    
To solidify our understanding of the distinction between correlation and convolution, we will process another image.
    
\begin{subquestion}[points=4]
    Devise a scenario in which the output of correlation and convolution differ.
    
    Write code that loads an image and produces two distinct images, one from convolution and one from correlation on some kernel of your choice. Then, compute the difference of the two images (the order in which you subtract the images should not matter) and display it as well.
    
    Specify your kernel, and provide the input image and two output results. Then, use your understanding of convolution and correlation to explain the outputs. \textbf{[2--4 sentences]}
\end{subquestion}
    
\emph{Consider \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html}{$scipy.signal.convolve2d$} and \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate2d.html}{$scipy.signal.correlate2d$} to experiment!}
    

\begin{answer}
    \includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_orig_img.png}
    \includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_difference.png}\\
    \includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_conv_res.png}
    \includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_corr_res.png}
    
    TODO: Your explanation here.
\end{answer}

\pagebreak

\begin{subquestion}[points=1]
Consider a situation where we apply two different filters sequentially to an image. How will the output image change depending on the order in which we apply the filters? Will the behavior be different for convolution versus correlation? \textbf{[1--2 sentences]}
\end{subquestion}

\begin{answer}[height=14]
    
\end{answer}

\pagebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a CSCI 0410/basic ML course knowledge transfer question.
% Apply what you learned in your intro AI course!
%
\begin{question}[points=10,drawbox=false]
\textbf{Differentiating convolution.} Convolution as an operation is defined only by multiplications and additions of the values within the filter and the image inputs. When interpreting these values as real $\mathbb{R}$ numbers (floating point) and not integers $\mathbb{Z}$, addition and multiplication are smooth functions. Thus, we can analytically differentiate convolution to compute a derivative with respect to its inputs and outputs, such that we can use continuous optimization methods to minimize an objective defined in those terms.

\emph{Of course,} even though we might use continuous values within, discrete convolution is still defined upon the discrete pixel grid---we cannot analytically differentiate with respect to grid size terms, e.g., the kernel width or height, which are still integers. We can only analytically differentiate with respect to the values within the kernel.

So, given a target image and an original image, suppose we wish to find the kernel that produced the target image. Let $T \in \mathbb{R}_{M\times N}$ be the target image, $I \in \mathbb{R}_{M\times N}$ be our original image, and our kernel $f \in \mathbb{R}_{K \times L}$. As a reminder, convolution is defined as:
$$H = f * I =  \sum_{k,l}{f[k,l]I[m-k,n-l]}$$
\end{question}

\begin{subquestion}[points=1] 
Write down a mean-squared error (MSE) loss function $\mathcal{L}$ between the convolved original image and the target, where the mean is over all pixels. Include summation terms over $M, N$ and index pixels with $m, n$.
\end{subquestion}
\begin{answer}[height=8]
$$\mathcal{L} = \frac{1}{MN} \sum_{m=1}^{M} \sum_{n=1}^{N} ((f * I)[m,n] - T[m,n] )^2$$
$$\mathcal{L} = \frac{1}{MN} \sum_{m=1}^{M} \sum_{n=1}^{N} (H[m,n] - T[m,n] )^2$$
\end{answer}

\pagebreak

\begin{subquestion}[points=7]
Next, find an equation for the partial derivative of the MSE loss with respect to one of the filter weights:
$$\frac{\partial{\mathcal{L}}}{\partial{f[k,l]}}$$

\begin{enumerate}
\item This will require an application of the chain rule as this derivative depends upon the convolution output $H$. Start here.
\item As $f[k,l]$ contributes to every pixel in the output $H$ and so every pixel contributes to $\mathcal{L}$, your application must sum the gradients over all output pixels $M, N$.
\item When differentiating the loss term wrt. $H$, recall that the loss itself is defined as a sum over all pixels---use dummy iterators, e.g., $m',n'$ to keep track. But, we're taking the derivative with respect to a specific pixel location, so what happens at those other locations?
\end{enumerate}
\end{subquestion}
\begin{answer}[height=28]
\end{answer}
% Use two answer boxes if it goes over the page
\begin{answer}[height=47]
Apply the chain rule:
$$\frac{\partial{\mathcal{L}}}{\partial{f[k,l]}} = \sum_{m=1}^{M} \sum_{n=1}^{N} \frac{\partial{\mathcal{L}}}{\partial{H[m,n]}} \cdot \frac{\partial{H[m,n]}}{\partial{f[k,l]}}$$

Consider the first term (outer derivative), and substitute the loss into the equation. Note that we must introduce two sets of $m,n$ variables: $m,n$ index the specific pixel we are differentiating with respect to, and $m',n'$ are iterators that cover the whole image to compute the loss.
$$\frac{\partial{\mathcal{L}}}{\partial{H[m,n]}} = \frac{\partial}{\partial{H[m,n]}} \left[ \frac{1}{MN} \sum_{m'=1}^{M} \sum_{n'=1}^{N} (H[m',n']-T[m',n'])^2 \right] $$

For all entries where $m \neq m'$ and $n \neq n'$, the derivative will be 0---the error at some other pixel does not affect the error at the pixel we care about. Thus, 
$$\frac{\partial{\mathcal{L}}}{\partial{H[m,n]}} = \frac{\partial}{\partial{H[m,n]}} \left[ \frac{1}{MN} (H[m,n]-T[m,n])^2 \right] $$
$$\frac{\partial{\mathcal{L}}}{\partial{H[m,n]}} = \frac{2}{MN} (H[m,n]-T[m,n]) $$
-----

Now consider the second term (inner derivative), and substitute in the equation for convolution. We introduce dummy variables $k',l'$ similarly.
$$\frac{\partial{H[m,n]}}{\partial{f[k,l]}} = \frac{\partial}{\partial{f[k,l]}} \sum_{k',l'}{f[k',l']I[m-k',n-l']} $$

Again, for all entries where $k \neq k'$ and $l \neq l'$, the derivative is 0. Thus,
$$\frac{\partial{H[m,n]}}{\partial{f[k,l]}} = \frac{\partial}{\partial{f[k,l]}} {f[k,l]I[m-k,n-l]} $$
Since $I[m-k,n-l]$ acts as a constant coefficient with respect to $f[k,l]$, the derivative of the product is simply the coefficient:
$$\frac{\partial{H[m,n]}}{\partial{f[k,l]}} = I[m-k,n-l]$$
-----

Substituting both terms back in: 

$$\frac{\partial{L}}{\partial{f[k,l]}}= \sum_{m=1}^{M} \sum_{n=1}^{N} \left( \frac{2}{MN} (H[m,n]-T[m,n]) \cdot I[m-k, n-l] \right)$$
\end{answer}

\pagebreak

\begin{subquestion}[points=2]
Given what we've learned about filtering so far, provide an interpretation of your definition of $\frac{\partial{L}}{\partial{f[k,l]}}$. What is it?
\end{subquestion}

\begin{answer}
This has a clean interpretation: the gradient for convolution MSE at a kernel pixel location \emph{is} the correlation (or cross-correlation) between the (scaled) error map and the input image shifted by that kernel pixel location offset. This can be more easily seen if we think about $H, T, I$ as whole matrices `outside' the summation rather than as individual pixels inside it. $I$ is offset by $k,l$, and is correlated with the (scaled) error $H-T$.
\end{answer}

\pagebreak

Convolution with an MSE loss is convex: MSE is a sum of squares (parabola, with a single valley), and since convolution is a linear operator it can only stretch or rotate the parabola. Convolution can never create a `second valley' within this parabola.

Seeing convolution as a linear operator provides additional interpretations. Suppose we again wish to recover a kernel $f$ given a target and original image, but our input or target image had noise. We could form linear systems like $Ax = b$ to solve least squares regression problems that find a kernel of best fit.

$$f * I = \sum_{k,l}{f[k,l]I[m-k,n-l]} = Ax = T$$

Each row of $A$ would be a 2D patch of $I$ that have been flattened, such that $A \in \mathbb{R}^{(m \cdot n) \times (k \cdot l)} $. $x$ would be the flattening of $f$ into a column vector, and $b$ would be the corresponding flattening of $T$ into a vector, such that multiplication $Ax$ produces the result of the convolution $T$ in its corresponding vector of elements $b$.

\begin{subquestion}[points=2]
Write pseudocode that creates matrix A.
\end{subquestion}

\begin{answer}[height=12]
\begin{python}
for m in range(H):
    for n in range(W):
        # flatten patch 
        patch = I[m:m+K, n:n+L].flatten()
        # set row to equal the patch 
        A[row, :] = patch
        row += 1
\end{python}
\end{answer}




\pagebreak % Please leave the page break
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=6,drawbox=false]
While exploring Brown CS's history in the halls of CIT, we happen upon Nancy: a DEC VAX 11/780. So struck by its beauty, we decide to take an artful photo with our camera. Modern digital sensors have many megapixels, so we resize it to make the file smaller.

\begin{tabular}{c c}
\includegraphics[width=0.49\textwidth,height=7cm,keepaspectratio]{images/poor_nancy_markup.png} &
\includegraphics[width=0.49\textwidth,height=7cm,keepaspectratio]{images/poor_nancy_better.png} \\
Resized image & Depiction of original (\emph{not original file})
\end{tabular}

Oh no! What happened to Nancy? There are weird artifacts in the vents (above red line)---these definitely weren't there in the original photo. Plus, if we look closely, the white label text is less smooth and there are jagged lines.
\end{question}

\begin{subquestion}[points=3]
What is this phenomenon called, and why did it happen? \textbf{[2--4 sentences]}
\end{subquestion}

\begin{answer}[height=8]

\end{answer}

\begin{subquestion}[points=3]
How might we fix this issue with filtering? Describe the process, and explain why it works. \textbf{[2--4 sentences]}
\end{subquestion}

\begin{answer}[height=8]

\end{answer}

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=14,drawbox=false]
With filtering, we can create \emph{hybrid images} that depict different objects when viewed at different distances. They are inauthentic images of the natural world.

As technology advances, evaluating the authenticity of images becomes increasingly difficult. Please read \href{https://www.nytimes.com/1990/08/12/arts/photography-view-ask-it-no-questions-the-camera-can-lie.html}{this article} by photography critic Andy Grundberg in the \emph{New York Times} from August 1990.

Grundberg stated that: ``In the future, readers of newspapers and magazines will probably view news pictures more as illustrations than as reportage, since they can no longer distinguish between a genuine image and one that has been manipulated.''
\end{question}

\begin{subquestion}[points=4]
When is Grundberg's future, and why? \textbf{[4--6 sentences]}
\end{subquestion}

\begin{answer}[height=11]

\end{answer}

\begin{subquestion}[points=4]
For a news picture, are any digital manipulations permissible? If so, how do we decide which ones? Use at least one ethical framework from the \href{https://browncsci1430.github.io/resources/ethics_primer/}{ethics primer} to explore this. \textbf{[4--6 sentences]}
\end{subquestion}

\begin{answer}[height=11]

\end{answer}
    
\pagebreak

The Coalition for Content Provenance and Authenticity (C2PA) has designed a technical specification to attach history to an image. Please watch \href{https://www.youtube.com/watch?v=hA0ZjqakEF8}{this video} for an overview; stop at 4 minutes and 40 seconds.

For context, a stakeholder of a system is identified as a person or group who has an explicit interest or concern in the system itself, its operations, and its consequences.
    
\begin{subquestion}[points=4]
Describe one situation in which the C2PA system helps us in determining authenticity and identify one stakeholder that benefits. Then, describe a second situation in which C2PA does not help to determine authenticity and one stakeholder that is still at risk. Please explain why C2PA helps or not in each case. \textbf{[4--6 sentences]}
\end{subquestion}

\begin{answer}[height=12]

\end{answer}

\begin{subquestion}[points=2]
Grundberg's article is titled ``Ask It No Questions: The Camera Can Lie.''

Does the C2PA system weaken Grundberg's argument? \textbf{[1--2 sentences]}
\end{subquestion}

\begin{answer}[height=8]

\end{answer}

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=0,drawbox=false]
\textbf{Technical practice (optional).} In computer vision, each image is a matrix of pixels. The \texttt{numpy} library provides fast computation with large multi-dimensional vectors and matrices. 

To familiarize yourself with the library, read through the following scenarios and complete the exercises. It is possible to use \emph{one} \texttt{numpy} function to complete each of the following tasks.

With \texttt{numpy} imported as
\begin{verbatim}
    import numpy as np
\end{verbatim}
we can call functions with 
\begin{verbatim}
    np.function_name(<arguments>)
\end{verbatim}
Test out your answers by creating your own python program. Some functions you might find useful are \href{https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html}{np.squeeze}, \href{https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html}{np.expand\_dims}, \href{https://numpy.org/doc/stable/reference/generated/numpy.clip.html}{np.clip}, \href{https://numpy.org/doc/stable/reference/generated/numpy.pad.html}{np.pad}, and \href{https://numpy.org/doc/stable/reference/generated/numpy.zeros.html}{np.zeros}.

Use operators like \texttt{[]} and \texttt{:}, but remember that each prompt can be completed with only \textit{one} function/operator shorthand.
\end{question}

\begin{subquestion}
Create a black image \texttt{img} with all values in this matrix equal to 0, where \texttt{np.shape(img) == (320,640)}.
\end{subquestion}
    
\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}
    
\begin{subquestion}
Assume we have a 2D matrix \texttt{img} with values range [-1.0, 1.0]. Clip \texttt{img} so that all its values lie within the range [-0.5, 0.5].
\end{subquestion}

\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}

\pagebreak

\begin{subquestion}
A malformed filter operation has messed up the output dimensions, producing a variable \texttt{img\_out} where \texttt{np.shape(img\_out) == (1, 1, 320, 640)}. Remove all 1-sized dimensions. Convert \texttt{img\_out} to a new matrix \texttt{img\_fixed} where \texttt{np.shape(img\_fixed) == (320, 640)}.
\end{subquestion}

\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}

Color images are represented with a three dimensional matrix, where often the third dimension represents spectral information. The presence of a third dimension or the size of the color dimension could help us to identify whether an image is color (RGB) or grayscale.

\begin{subquestion}
Say you have a grayscale image \texttt{img} where \texttt{np.shape(img) == (320, 640)}. Convert this to a new image \texttt{img\_expanded} where \texttt{np.shape(img\_expanded) == (320, 640, 1)}. In other words, add a 1-sized dimension to \texttt{img}.
\end{subquestion}

\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}
    
\begin{subquestion}
Suppose we have an RGB image matrix, \texttt{img}, of shape (320, 640, 3). Retrieve the third blue channel of the image while preserving all of \texttt{img}'s dimensions and intensity values.
\end{subquestion}
    
\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}

\pagebreak

\begin{subquestion}
Suppose we have a second RGB image matrix, \texttt{img2}, also of shape (320, 640, 3). Retrieve the red and blue channels of \texttt{img2} within a single variable of shape (320, 640, 2).
\end{subquestion}
    
\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}
    
\begin{subquestion}
Padding is a useful operation to help us produce a convolved image equal in size to an input image. Given an RGB image, \texttt{img} of (320, 640, 3), pad it with two columns of zeros on the left and right edges of the image, and three rows of zeros on the top and bottom edges of the image. Do not add zero padding to the color dimension.
\end{subquestion}
    
\begin{answer}
\begin{python}
# TODO: Your expression here
\end{python}
\end{answer}

\pagebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\writefeedback

\end{document}
